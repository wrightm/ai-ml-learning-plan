# AI & ML Learning â€“ Agentic AI + Recommenders (1 hour/day for 52 weeks)

<!--PROGRESS_BADGE_START-->
![Progress](https://img.shields.io/badge/Progress-0%25-brightgreen)
<!--PROGRESS_BADGE_END-->

## ðŸ’ª One Hour a Day - Arnold Schwarzenegger

> *"The worst thing I can be is the same as everybody else. I hate that."* - Arnold Schwarzenegger

Click to watch Arnold's motivation on the power of one hour a day:

[![Arnold Schwarzenegger - One Hour a Day](https://img.youtube.com/vi/mPo-YY5kMOA/maxresdefault.jpg)](https://www.youtube.com/watch?v=mPo-YY5kMOA "One Hour a Day - Arnold Schwarzenegger")

**The message**: One hour a day, dedicated and focused, is all it takes to transform yourself. No excuses.

---

## The Plan

A ruthless, focused plan for a senior staff engineer (math/physics background, a bit rusty) to become **expert in Agentic AI** and **Recommendation Systems** in one year with ~**1 hour/day**.

> **Cadence**: Monâ€“Fri â†’ ~60m/day (theory, paper skim, coding, experiment, weekly deliverable). Use `journal/weeks/WEEK_XX/` to log progress.

## Repo layout
```
.
â”œâ”€ README.md                    # This file - your learning plan
â”œâ”€ SETUP.md                     # Detailed setup guide
â”œâ”€ QUICK_REFERENCE.md           # Command cheat sheet
â”œâ”€ requirements.txt             # Python dependencies
â”œâ”€ Makefile                     # Convenient commands
â”œâ”€ journal/weeks/               # Your weekly work goes here
â”‚  â””â”€ WEEK_XX/
â”‚     â”œâ”€ README.md              # Week notes
â”‚     â”œâ”€ *.ipynb                # Jupyter notebooks
â”‚     â””â”€ *.py                   # Python utilities
â”œâ”€ courses/                     # Course references
â”œâ”€ certs/                       # Certification guides
â””â”€ scripts/
   â””â”€ update_badge.py           # Updates progress badge
```

> Track your progress by checking boxes below as you complete each week.

---

## Getting started

### 1. Python Environment Setup
This project requires **Python 3.11+**. Quick start:

```bash
# Create and activate virtual environment
python3.11 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# .venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Verify setup
python verify_setup.py
```

> See [SETUP.md](SETUP.md) for detailed installation instructions and troubleshooting.

### 2. Start Learning
```bash
# Launch Jupyter Lab
jupyter lab
# OR use Makefile
make lab
```

Your browser will open. Navigate to `journal/weeks/WEEK_01/W01_day_by_day.ipynb` to start!

### 3. Weekly Workflow
- **Mark progress**: Check boxes below as you complete weeks (`- [ ]` â†’ `- [x]`)
- **Take notes**: Document learnings in `journal/weeks/WEEK_XX/README.md`
- **Code daily**: Aim for ~60 minutes, Mondayâ€“Friday
- **Commit work**: Push to GitHub to track your progress

> **Optional**: Set up [GitHub Actions](.github/) to auto-update the progress badge above.

---

## 52â€‘Week Plan at a Glance
**Weekly rhythm**
- **Mon:** theory (videos/reading) ~60m  
- **Tue:** skim & notes (papers/blogs) ~60m  
- **Wed:** code from scratch (notebooks/scripts) ~60m  
- **Thu:** extend/experiment ~60m  
- **Fri:** mini-deliverable (writeup, PR, or result) ~60m  

**Core Stack**
- **Foundation**: Python 3.11+, NumPy, Pandas, Matplotlib, scikit-learn, Jupyter
- **Deep Learning**: PyTorch, Lightning
- **Experimentation**: Weights & Biases
- **Recommender Systems**: implicit, LightFM, FAISS
- **LLM & Agents**: LangChain, OpenAI API, instructor
- **Later phases**: FastAPI (serving), SQLAlchemy (data)

---

## Phase 1 â€” Math Reboot & ML Foundations (Weeks 1â€“12)
- [ ] **W1 â€” Vectors & Matrices refresher** â†’ PCA via SVD notebook
- [ ] **W2 â€” Probability & Stats I** â†’ CLT simulation & sampling demos
- [ ] **W3 â€” Probability & Stats II** â†’ A/B test notebook + power analysis
- [ ] **W4 â€” Optimization** â†’ Logistic regression from scratch (GD)
- [ ] **W5 â€” ML Experiment Tracking** â†’ W&B setup, logging experiments, comparing runs
- [ ] **W6 â€” Supervised I** â†’ Linear/regularized models baseline
- [ ] **W7 â€” Supervised II** â†’ Trees/GBM comparison + SHAP
- [ ] **W8 â€” Unsupervised** â†’ Clustering + visualization
- [ ] **W9 â€” Evaluation** â†’ Metrics utils (ROC/PR, calibration, ranking)
- [ ] **W10 â€” Data/Serving** â†’ FastAPI model service (artifactâ€‘pinned)
- [ ] **W11 â€” Experimentation** â†’ Ablation template + example
- [ ] **W12 â€” Checkpoint 1** â†’ Rebuild LR + GBM; 2â€“3p memo

## Phase 2 â€” Deep Learning & Representations (Weeks 13â€“20)
- [ ] **W13 â€” PyTorch fundamentals** â†’ MLP on MNIST/CIFAR with clean loops
- [ ] **W14 â€” Optimization in practice** â†’ Recipe card reaching target accuracy
- [ ] **W15 â€” Embeddings & metric learning** â†’ Contrastive toy + retrieval
- [ ] **W16 â€” Sequence models** â†’ Nextâ€‘item predictor (synthetic data)
- [ ] **W17 â€” Transformers** â†’ Fineâ€‘tune small LM; eval perplexity
- [ ] **W18 â€” Retrieval & Vector Search** â†’ Retrieval API over embeddings
- [ ] **W19 â€” Serving & Profiling** â†’ Latency/QPS benchmarks; FastAPI endpoint
- [ ] **W20 â€” Checkpoint 2** â†’ Write â€œWhat I know about embeddings & sequencesâ€

## Phase 3 â€” Recommendation Systems (Weeks 21â€“32)
- [ ] **W21 â€” Framing & Metrics** â†’ Ranking harness (AUC/MAP/NDCG)
- [ ] **W22 â€” Nearestâ€‘neighbor CF** â†’ Topâ€‘K baseline + caching
- [ ] **W23 â€” Matrix Factorization** â†’ MF (ALS/SGD) + BPR; compare to W22
- [ ] **W24 â€” FMs & Wideâ€‘andâ€‘Deep** â†’ Sparse features vs MF
- [ ] **W25 â€” Twoâ€‘Tower Retrieval** â†’ Neg sampling; Recall@K lift
- [ ] **W26 â€” Reâ€‘ranking** â†’ GBM/transformer reâ€‘ranker; NDCG lift
- [ ] **W27 â€” Sequential Recsys** â†’ SASRec/GRU4Rec ablations
- [ ] **W28 â€” Explore/Exploit** â†’ Bandits; offline IPS/DR evaluation
- [ ] **W29 â€” Causality/Uplift (intro)** â†’ Toy uplift & product notes
- [ ] **W30 â€” Practical Evaluation** â†’ Guardrails; diversity/serendipity metrics
- [ ] **W31 â€” Coldâ€‘start & Taxonomy** â†’ Content features & hierarchy
- [ ] **W32 â€” Checkpoint 3 (Miniâ€‘Capstone #1)** â†’ Retrievalâ†’reâ€‘rank pipeline + API

## Phase 4 â€” Agentic AI (Weeks 33â€“42)
- [ ] **W33 â€” Agent architectures** â†’ Compare planner/executor, debate, graphs
- [ ] **W34 â€” Tool use & function calling** â†’ 3 tools + schema validation
- [ ] **W35 â€” Multiâ€‘agent patterns** â†’ Supervisor/worker demo
- [ ] **W36 â€” Memory & reflection** â†’ Persistent memory; staleness tests
- [ ] **W37 â€” Planning** â†’ Multiâ€‘step task planner (simulated web)
- [ ] **W38 â€” RAG done right** â†’ Evaluation (faithfulness, relevancy)
- [ ] **W39 â€” Agent evaluation** â†’ Trace KPIs; harness
- [ ] **W40 â€” Safety & reliability** â†’ Guardrails; PII; timeouts/circuit breakers
- [ ] **W41 â€” Local vs hosted** â†’ Adapter to swap LLM backends
- [ ] **W42 â€” Checkpoint 4 (Miniâ€‘Capstone #2)** â†’ Taskâ€‘oriented agent demo

## Phase 5 â€” Integrating Agents + Recsys (Weeks 43â€“48)
- [ ] **W43 â€” Recommendation agent** â†’ Offline evals + config PRs
- [ ] **W44 â€” Conversational recsys** â†’ Preference updates & reâ€‘ranking
- [ ] **W45 â€” Exploration agent** â†’ Autoâ€‘tagging + A/B proposals
- [ ] **W46 â€” Bandit + agent loop** â†’ Simulated regret reduction
- [ ] **W47 â€” MLOps hardening** â†’ Feature store, drift, CI/CD, canary
- [ ] **W48 â€” Checkpoint 5** â†’ Operating Manual for your platform

## Phase 6 â€” Capstone & Portfolio (Weeks 49â€“52)
- [ ] **W49â€“W51 â€” Capstone** â†’ Pick 1 (or both): Recsys or Agentic platform
- [ ] **W52 â€” Polish** â†’ Slides, blog(s), demo video, portfolio

---

## Journal Structure
Each week, work in `journal/weeks/WEEK_XX/`:
```md
WEEK_XX/
â”œâ”€â”€ README.md           # Week notes and learnings
â”œâ”€â”€ *.ipynb             # Jupyter notebooks with implementations
â”œâ”€â”€ *.py                # Reusable Python utilities
â””â”€â”€ reading_list.md     # Papers and resources
```

**Example Week Notes** (`README.md`):
```md
# Week 01 â€” Vectors & Matrices
**Goals:** PCA via SVD; refresh linear algebra fundamentals  
**What I Learned:** SVD decomposition, explained variance, whitening  
**Code:** W01_day_by_day.ipynb  
**Key Insights:** PCA is just eigendecomposition of covariance...  
**Next Week:** Probability & stats refresher
```

## Tips for Success
- **Consistency > intensity**: 1 hour/day beats 7 hours on Sunday
- **Code everything from scratch**: Understanding > efficiency when learning
- **Visualize concepts**: Plots build intuition
- **Document as you go**: Future you will be grateful
- **Review previous weeks**: Spaced repetition aids retention
- **Don't skip fundamentals**: Strong foundations enable advanced work

---

### Credits / Reading List (sprinkle across the year)
- Math: *Matrix Cookbook*, Murphyâ€™s *ML: A Probabilistic Perspective* (selected), Boyd & Vandenberghe (convex chapters).
- Recsys: *Recommender Systems Handbook* (chapters), Rendle (BPR), He et al. (Neural CF), Hidasi et al. (GRU4Rec), Kang & McAuley (SASRec), Covington et al. (YouTube).
- Agents & RAG: LangGraph docs, function-calling & structured outputs, multi-agent surveys, retrieval evaluation guides.
- MLOps: Chip Huyen, blog posts on drift/lineage/feature stores.

---

Happy buildingâ€”check a box each week and ship something small every Friday.
