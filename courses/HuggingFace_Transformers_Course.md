# HuggingFace Transformers Course

**Why:** Tokenization, transformers, PEFT/LoRA.

**Suggested slot:** Weeks 17–18

## What to do
- [ ] Fine-tune with LoRA
- [ ] Export to ONNX and benchmark
- [ ] Compare cosine vs dot for search

## Deliverable
- Short README/notebook proving outcomes.

## Mapping to the 52-Week Plan
- **Weeks:** 17–18  
- **Phase:** Phase 2 — Deep Learning & Representations
